Title,Publication,Bit,Quantizer,Finetune,Task,Special
Fully integer-based quantization for mobile convolutional neural network inference,Neurocomputing 2021,,,,,
S^3: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks,NeurIPS 2021,T,,,C,
BatchQuant: Quantized-for-all Architecture Search with Robust Quantizer,NeurIPS 2021,MP,,QAT,C,
CBP: backpropagation with constraint on weight precision using a pseudo-Lagrange multiplier method[PyTorch],NeurIPS 2021,B/T/Uni,,QAT,C,
"Divergence Frontiers for Generative Models: Sample Complexity, Quantization Effects, and Frontier Integrals",NeurIPS 2021,B/Uni,,,C,
Learning Frequency Domain Approximation for Binary Neural Networks,NeurIPS 2021,B,,QAT,C,
Post-Training Quantization for Vision Transformer,NeurIPS 2021,Uni,,PTQ,C,
Post-Training Sparsity-Aware Quantization[PyTorch]:star:5,NeurIPS 2021,Uni,,PTQ,C,
Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples[PyTorch]:star:2,NeurIPS 2021,Uni,,,C,
Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving Adversarial Outcomes[PyTorch]:star:2,NeurIPS 2021,Uni,,QAT,C,
QuPeD: Quantized Personalization via Distillation with Applications to Federated Learning,NeurIPS 2021,B/Uni,,QAT,C,
VQ-GNN: A Universal Framework to Scale up Graph Neural Networks using Vector Quantization,NeurIPS 2021,MP,PQ,QAT,Node Classification,
Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression,EMNLP 2021,Uni,,QAT/PTQ,N,
Compressing Word Embeddings via Deep Compositional Code Learning:fire:89[PyTorch]:star:81,EMNLP 2021,MP,,,N,
Matching-oriented Embedding Quantization For Ad-hoc Retrieval[PyTorch],EMNLP 2021,MP,PQ,QAT,N,
Understanding and Overcoming the Challenges of Efficient Transformer Quantization[PyTorch]:star:9,EMNLP 2021,Uni,,QAT/PTQ,N,
Fully Quantized Image Super-Resolution Networks,MM 2021,,,,,
VQMG: Hierarchical Vector Quantised and Multi-hops Graph Reasoning for Explicit Representation Learning,MM 2021,,,,,
Cluster-Promoting Quantization With Bit-Drop for Minimizing Network Quantization Loss,ICCV 2021,T/Uni,LQ,,C,
Distance-Aware Quantization,ICCV 2021,B/T/Uni,LQ,QAT,C,
Dynamic Network Quantization for Efficient Video Inference,ICCV 2021,MP,,,Video Recognition,
Generalizable Mixed-Precision Quantization via Attribution Rank Preservation[PyTorch]:star:15,ICCV 2021,MP,LQ,QAT,C,
Improving Low-Precision Network Quantization via Bin Regularization,ICCV 2021,B/T/Uni,LQ,QAT,C,
Improving Neural Network Efficiency via Post-Training Quantization With Adaptive Floating-Point[PyTorch],ICCV 2021,MP,Linear,PTQ,C,
Integer-Arithmetic-Only Certified Robustness for Quantized Neural Networks,ICCV 2021,Uni,Linear,QAT,C,
MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing,ICCV 2021,Uni,Linear,QAT/PTQ,C/O,
Once Quantization-Aware Training: High Performance Extremely Low-Bit Architecture Search[PyTorch]:star:19,ICCV 2021,Uni,,QAT,C,
Product Quantizer Aware Inverted Index for Scalable Nearest Neighbor Search,ICCV 2021,MP,PQ,QAT,C,
RMSMP: A Novel Deep Neural Network Quantization Framework With Row-Wise Mixed Schemes and Multiple Precisions,ICCV 2021,MP,,QAT/PTQ,C/N,
ReCU: Reviving the Dead Weights in Binary Neural Networks,ICCV 2021,B,,,C,
Self-Supervised Product Quantization for Deep Unsupervised Image Retrieval[PyTorch]:star:22,ICCV 2021,Uni,PQ,QAT,C,
Sub-bit Neural Networks: Learning to Compress and Accelerate Binary Neural Networks[PyTorch]:star:7,ICCV 2021,B,,,C,
Towards Mixed-Precision Quantization of Neural Networks via Constrained Optimization,ICCV 2021,MP,Linear,PTQ,C,
1-bit Adam: Communication Efficient Large-Scale Training with Adam’s Convergence Speed,ICML 2021,B,,,C/N,
Accurate Post Training Quantization With Small Calibration Sets[PyTorch]:star:14,ICML 2021,MP,,PTQ,C/N,
ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training,ICML 2021,MP,,,C,
Communication-Efficient Distributed Optimization with Quantized Preconditioners,ICML 2021,,PQ,,,
Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution,ICML 2021,MP,,QAT,C,
Double-Win Quant: Aggressively Winning Robustness of Quantized Deep Neural Networks via Random Precision Training and Inference[PyTorch]:star:3,ICML 2021,MP,,,C,
Estimation and Quantization of Expected Persistence Diagrams,ICML 2021,,,,,
HAWQ-V3: Dyadic Neural Network Quantization[PyTorch]:star:193,ICML 2021,MP,Linear,,C,
I-BERT: Integer-only BERT Quantization,ICML 2021,Uni,,,N,
Quantization Algorithms for Random Fourier Features,ICML 2021,,,,,
Soft then Hard: Rethinking the Quantization in Neural Image Compression,ICML 2021,,,,,
Training Quantized Neural Networks to Global Optimality via Semidefinite Programming,ICML 2021,,,,,
Vector Quantized Models for Planning,ICML 2021,,,,,
Distribution-aware Adaptive Multi-bit Quantization,CVPR 2021,,,,,
Layer importance estimation with imprinting for neural network quantization,CVPR 2021,,,,,
Adaptive binary-ternary quantization,CVPR 2021,,,,,
AQD: Towards Accurate Quantized Object Detection[PyTorch]:star:11,CVPR 2021,,,,,
Automated Log-Scale Quantization for Low-Cost Deep Neural Networks,CVPR 2021,T,Log,QAT,C/S,
Binary TTC: A Temporal Geofence for Autonomous Navigation,CVPR 2021,,,,,
Diversifying Sample Generation for Accurate Data-Free Quantization,CVPR 2021,,,,,
Generative Zero-shot Network Quantization,CVPR 2021,Uni,OptN,QAT,C,
Improving Accuracy of Binary Neural Networks using Unbalanced Activation Distribution,CVPR 2021,,,,,
Is In-Domain Data Really Needed? A Pilot Study on Cross-Domain Calibration for Network Quantization,CVPR 2021,Uni,OptN,PTQ,C/O/S,
Learnable Companding Quantization for Accurate Low-Bit Neural Networks,CVPR 2021,,,,,
Network Quantization With Element-Wise Gradient Scaling,CVPR 2021,,,,,
Optimal Quantization Using Scaled Codebook,CVPR 2021,,,,,
"Permute, Quantize, and Fine-tune: Efficient Compression of Neural Networks[PyTorch]:star:109",CVPR 2021,,,,,
QPP: Real-Time Quantization Parameter Prediction for Deep Neural Networks,CVPR 2021,Uni,LQ,PTQ,C/O/S,
Zero-shot Adversarial Quantization,CVPR 2021,Uni,Linear,PTQ,C/O,
iVPF: Numerical Invertible Volume Preserving Flow for Efficient Lossless Compression,CVPR 2021,,,,,
HAO: Hardware-aware neural Architecture Optimization for Efficient Inference,FCCM 2021,,,,,
Bipointnet: Binary neural network for point clouds,ICLR 2021,,,,,
Sparse quantized spectral clustering,ICLR 2021,,,,,
Neural gradients are near-lognormal: improved quantized and sparse training,ICLR 2021,,,,,
Multiprize lottery ticket hypothesis: Finding accurate binary neural networks by pruning a randomly weighted network,ICLR 2021,,,,,
High-capacity expert binary networks,ICLR 2021,,,,,
BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction[PyTorch]:star:58,ICLR 2021,Uni/MP,Linear,PTQ,C/O,
BSQ: Exploring Bit-Level Sparsity for Mixed-Precision Neural Network Quantization[PyTorch]:star:9,ICLR 2021,MP,Linear,QAT,C,
Degree-Quant: Quantization-Aware Training for Graph Neural Networks[PyTorch]:star:17,ICLR 2021,Uni,Linear,QAT,C,
Incremental few-shot learning via vector quantization in deep embedded space,ICLR 2021,,,,,
Simple Augmentation Goes a Long Way: ADRL for DNN Quantization,ICLR 2021,MP,Linear,QAT,C,
Training with Quantization Noise for Extreme Model Compression,ICLR 2021,Uni,PQ,QAT,C,
CoDeNet: Algorithm-hardware Co-design for Deformable Convolution,FPGA 2021,,,,,
A Survey of Quantization Methods for Efficient Neural Network Inference:fire:47,BLPCV 2021,,,,,
Opq: Compressing deep neural networks with one-shot pruning quantization,AAAI 2021,,,,,
A white paper on neural network quantization,arXiv 2021,,,,,
Kdlsq-bert: A quantized bert combining knowledge distillation with learned step size quantization,arXiv 2021,,,,,
Pruning and quantization for deep neural network acceleration: A survey,arXiv 2021,,,,,
Confounding tradeoffs for neural network quantization,arXiv 2021,,,,,
Dynamic precision analog computing for neural networks,arXiv 2021,,,,,
Boolnet: Minimizing the energy consumption of binary neural networks,arXiv 2021,,,,,
Quantization-aware pruning for efficient low latency neural network inference,arXiv 2021,,,,,
Any-Precision Deep Neural Networks[PyTorch]:star:33,arXiv 2021,MP,,,,
Phoenix: A Low-Precision Floating-Point Quantization Oriented Architecture for Convolutional Neural Networks,TVLSI 2020,,,,,
Hierarchical Binary CNNs for Landmark Localization with Limited Resources,TPAMI 2020,B,,,,
Deep Neural Network Compression by In-Parallel Pruning-Quantization,TPAMI 2020,,,,,
Towards Efficient U-Nets: A Coupled and Quantized Approach,TPAMI 2020,,,,,
SIMBA: A Skyrmionic In-Memory Binary Neural Network Accelerator,TMAG 2020,B,,,,
Design of High Robustness BNN Inference Accelerator Based on Binary Memristors,TED 2020,B,,,,
A Resource-Efficient Inference Accelerator for Binary Convolutional Neural Networks,TCSII 2020,,,,,
"Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations:fire:126",JSAIT 2020,,,,,Gradient
An Energy-Efficient and High Throughput in-Memory Computing Bit-Cell With Excellent Robustness Under Process Variations for Binary Neural Network,ACCESS 2020,B,,,,
CP-NAS: Child-Parent Neural Architecture Search for Binary Neural Networks,IJCAI 2020,B,,,,
Towards Fully 8-bit Integer Inference for the Transformer Model,IJCAI 2020,,,,,
Soft Threshold Ternary Networks,IJCAI 2020,,,,,
Overflow Aware Quantization: Accelerating Neural Network Inference by Low-bit Multiply-Accumulate Operations,IJCAI 2020,,,,,
Direct Quantization for Training Highly Accurate Low Bit-width Deep Neural Networks,IJCAI 2020,,,,,
Fully Nested Neural Network for Adaptive Compression and Quantization,IJCAI 2020,,,,,
Path sample-analytic gradient estimators for stochastic binary networks,NeurIPS 2020,,,,,
Efficient exact verification of binarized neural networks,NeurIPS 2020,,,,,
Comparing fisher information regularization with distillation for dnn quantization,NeurIPS 2020,,,,,
Position-based scaled gradient for model quantization and sparse training,NeurIPS 2020,,,,,
Flexor: Trainable fractional quantization,NeurIPS 2020,,,,,
Adaptive Gradient Quantization for Data-Parallel SGD[PyTorch]:star:13,NeurIPS 2020,T,,QAT,C,
Bayesian Bits: Unifying Quantization and Pruning,NeurIPS 2020,MP,,QAT/PTQ,C,
"Distribution-free binary classification: prediction sets, confidence intervals and calibration",NeurIPS 2020,B,,,,
FleXOR: Trainable Fractional Quantization,NeurIPS 2020,,,,,
HAWQ-V2: Hessian Aware trace-Weighted Quantization of Neural Networks:fire:60,NeurIPS 2020,MP,Linear,QAT,C/O,
Hierarchical Quantized Autoencoders[PyTorch]:star:22,NeurIPS 2020,,Linear,,Image Compression,
Position-based Scaled Gradient for Model Quantization and Pruning[PyTorch]:star:14,NeurIPS 2020,Uni,,PTQ,C,
Pushing the Limits of Narrow Precision Inferencing at Cloud Scale with Microsoft Floating Point,NeurIPS 2020,,,,,
Quantized Variational Inference[PyTorch],NeurIPS 2020,,,,,
Robust Quantization: One Model to Rule Them All,NeurIPS 2020,Uni,Linear,QAT/PTQ,C,
Rotated Binary Neural Network[PyTorch]:star:63,NeurIPS 2020,B,,,C,
Searching for Low-Bit Weights in Quantized Neural Networks[PyTorch]:star:20,NeurIPS 2020,,,,,
Ultra-Low Precision 4-bit Training of Deep Neural Networks,NeurIPS 2020,Uni,,QAT,C,
Universally Quantized Neural Compression,NeurIPS 2020,,,,,
TernaryBERT: Distillation-aware Ultra-low Bit BERT,EMNLP 2020,T,OptN,QAT,N,
DFQF: Data Free Quantization-aware Fine-tuning,ACML 2020,Uni,Linear,QAT,C,
One weight bitwidth to rule them all,ECCV 2020,,,,,
DBQ: A Differentiable Branch Quantizer for Lightweight Deep Neural Networks,ECCV 2020,MP,,,C,
Deep Transferring Quantization[PyTorch]:star:15,ECCV 2020,Uni,,,C,
Differentiable Joint Pruning and Quantization for Hardware Efficiency,ECCV 2020,MP,,,C,
Finding Non-Uniform Quantization Schemes using Multi-Task Gaussian Processes,ECCV 2020,MP,PQ,QAT,C,
Generative Low-bitwidth Data Free Quantization[PyTorch]:star:23,ECCV 2020,Uni,Linear,QAT,C,
HMQ: Hardware Friendly Mixed Precision Quantization Block for CNNs[PyTorch]:star:37,ECCV 2020,MP,LQ,QAT,C,
PAMS: Quantized Super-Resolution via Parameterized Max Scale,ECCV 2020,Uni,,,C,
Post-Training Piecewise Linear Quantization for Deep Neural Networks,ECCV 2020,T/Uni,Linear,PTQ,C,
QuEST: Quantized Embedding Space for Transferring Knowledge,ECCV 2020,,,,,
Quantization Guided JPEG Artifact Correction,ECCV 2020,,,,,
Search What You Want: Barrier Panelty NAS for Mixed Precision Quantization,ECCV 2020,MP,Linear,,C/O,
Task-Aware Quantization Network for JPEG Image Compression,ECCV 2020,,,,,
End to End Binarized Neural Networks for Text Classification,ACL 2020,B,,,,
Differentiable Product Quantization for End-to-End Embedding Compression[PyTorch]:star:38,ICML 2020,MP,PQ,QAT,N,
Don’t Waste Your Bits! Squeeze Activations and Gradients for Deep Neural Networks via TinyScript,ICML 2020,MP,,,C,
Moniqua: Modulo Quantized Communication in Decentralized SGD,ICML 2020,B/T,,,C,
Online Learned Continual Compression with Adaptive Quantization Modules[PyTorch]:star:19,ICML 2020,,,,,
Towards Accurate Post-training Network Quantization via Bit-Split and Stitching[PyTorch]:star:23,ICML 2020,T/Uni,OptN,PTQ,C/O,
Training Binary Neural Networks through Learning with Noisy Supervision,ICML 2020,B,,QAT,C,
Up or Down? Adaptive Rounding for Post-Training Quantization,ICML 2020,,,PTQ,,
Variational Bayesian Quantization[PyTorch]:star:19,ICML 2020,,,,,
Compressing deep neural networks on FPGAs to binary and ternary precision with HLS4ML,MLST 2020,,,,,
Balanced binary neural networks with gated residual,ICASSP 2020,,,,,
A Spatial RNN Codec for End-To-End Image Compression,CVPR 2020,,,,,
"APQ: Joint Search for Network Architecture, Pruning and Quantization Policy",CVPR 2020,MP,Linear,QAT,C,
AdaBits: Neural Network Quantization With Adaptive Bit-Widths,CVPR 2020,MP,Linear,PTQ,C,
Adaptive Loss-Aware Quantization for Multi-Bit Networks,CVPR 2020,MP,LQ,QAT,C,
Automatic Neural Network Compression by Sparsity-Quantization Joint Learning: A Constrained Optimization-Based Approach,CVPR 2020,MP,OptN,PTQ,C,
Central Similarity Quantization for Efficient Image and Video Retrieval[PyTorch]:star:161,CVPR 2020,Uni,Linear,QAT,C,
Data-Free Network Quantization With Adversarial Knowledge Distillation,CVPR 2020,Uni,Linear,QAT,C,
Forward and Backward Information Retention for Accurate Binary Neural Networks[PyTorch]:star:133,CVPR 2020,,,,,
Generalized Product Quantization Network for Semi-Supervised Image Retrieval,CVPR 2020,MP,LQ,QAT,C,
LSQ+: Improving low-bit quantization through learnable offsets and better initialization,CVPR 2020,MP,LQ,QAT,C,
M-LVC: Multiple Frames Prediction for Learned Video Compression[PyTorch]:star:51,CVPR 2020,,,,,
OctSqueeze: Octree-Structured Entropy Model for LiDAR Compression,CVPR 2020,,,,,
Structured Compression by Weight Encryption for Unstructured Pruning and Quantization,CVPR 2020,T,OptN,QAT,C,
Training Quantized Neural Networks With a Full-Precision Auxiliary Module,CVPR 2020,,,,,
ZeroQ: A Novel Zero Shot Quantization Framework:fire:106[PyTorch]:star:188,CVPR 2020,MP,Linear,PTQ,C/O,
Neural network quantization with adaptive bitwidths,CVPR 2020,,,,,
BNNsplit: Binarized Neural Networks for embedded distributed FPGA-based computing systems,DATE 2020,B,,,,
PhoneBit: Efficient GPU-Accelerated Binary Neural Network Inference Engine for Mobile Phones,DATE 2020,B,,,,
OrthrusPE: Runtime Reconfigurable Processing Elements for Binary Neural Networks,DATE 2020,B,,,,
A Novel In-DRAM Accelerator Architecture for Binary Neural Network,COOLCHIPS 2020,,,,,
BNN Pruning: Pruning Binary Neural Network Guided by Weight Flipping Frequency,ISQED 2020,B,,,,
Training binary neural networks with real-to-binary convolutions:fire:66,ICLR 2020,,,,,
Binaryduo: Reducing gradient mismatch in binary activation network by coupling binary activations,ICLR 2020,,,,,
Dms: Differentiable dimension search for binary neural networks,ICLR 2020,,,,,
Once-for-all: Train one network and specialize it for efficient deployment,ICLR 2020,,,,,
Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks[PyTorch]:star:150,ICLR 2020,MP,,,C,
And the Bit Goes Down: Revisiting the Quantization of Neural Networks:fire:64[PyTorch]:star:619,ICLR 2020,MP,PQ,,C,
AutoQ: Automated Kernel-Wise Neural Network Quantization,ICLR 2020,MP,,,C,
FSNet: Compression of Deep Convolutional Neural Networks by Filter Summary,ICLR 2020,Uni,Linear,PTQ,C/O,
Gradient $\ell_1$ Regularization for Quantization Robustness,ICLR 2020,Uni,Linear,,C,
Learning Hierarchical Discrete Linguistic Units from Visually-Grounded Speech,ICLR 2020,MP,PQ,,Speech,
Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware,ICLR 2020,B/T/Uni,LQ,QAT,C/O,
Mixed Precision DNNs: All you need is a good parametrization,ICLR 2020,MP,LQ,QAT,C,
Precision Gating: Improving Neural Network Efficiency with Dynamic Dual-Precision Activations,ICLR 2020,MP,,,C/N,
Shifted and Squeezed 8-bit Floating Point format for Low-Precision Training of Deep Neural Networks,ICLR 2020,Uni,,,C/N,
MuBiNN: Multi-Level Binarized Recurrent Neural Network for EEG Signal Classification,ISCS 2020,B,,,,
Riptide: Fast End-to-End Binarized Neural Networks,SysML 2020,,,,,
Adversarial Attack on Deep Product Quantization Network for Image Retrieval,AAAI 2020,,PQ,,Image Retrieval,
Aggregated Learning: A Vector-Quantization Approach to Learning Neural Network Classifiers[PyTorch]:star:3,AAAI 2020,,PQ,,C,
Embedding Compression with Isotropic Iterative Quantization,AAAI 2020,,,,N/Image Retrieval,
HLHLp: Quantized Neural Networks Training for Reaching Flat Minima in Loss Surface,AAAI 2020,B,Linear,QAT,C/N,
Indirect Stochastic Gradient Quantization and its Application in Distributed,AAAI 2020,,,,,
Norm-Explicit Quantization: Improving Vector Quantization for Maximum Inner Product Search,AAAI 2020,,,,,
Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT:fire:125,AAAI 2020,MP,Linear,QAT,N,
Quantized Compressive Sampling of Stochastic Gradients for Efficient Communication in Distributed Deep Learning,AAAI 2020,,,,,
RTN: Reparameterized Ternary Network,AAAI 2020,,,,,
Towards Accurate Low Bit-width Quantization with Multiple Phase Adaptations,AAAI 2020,,,,,
Towards Accurate Quantization and Pruning via Data-free Knowledge Transfer,AAAI 2020,MP,LQ,PTQ,C,
Vector Quantization-Based Regularization for Autoencoders[PyTorch]:star:11,AAAI 2020,,,,,
Training Binary Neural Networks using the Bayesian Learning Rule,CoRR 2020,B,,,,
Integer quantization for deep learning inference: Principles and empirical evaluation,arXiv 2020,,,,,
Wrapnet: Neural net inference with ultra-low-resolution arithmetic,arXiv 2020,,,,,
Leveraging automated mixed-low-precision quantization for tiny edge microcontrollers,arXiv 2020,,,,,
Biqgemm: matrix multiplication with lookup table for binary-coding-based quantized dnns,arXiv 2020,,,,,
Near-lossless post-training quantization of deep neural networks via a piecewise linear approximation,arXiv 2020,,,,,
Efficient execution of quantized deep learning models: A compiler approach,arXiv 2020,,,,,
A statistical framework for low-bitwidth training of deep neural networks,arXiv 2020,,,,,
What is the state of neural network pruning?,arXiv 2020,,,,,
Language models are fewshot learners,arXiv 2020,,,,,
Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks,arXiv 2020,,,,,
Gradient l1 regularization for quantization robustness,arXiv 2020,,,,,
BinaryBERT: Pushing the Limit of BERT Quantization,arXiv 2020,B,,,,
Understanding Learning Dynamics of Binary Neural Networks via Information Bottleneck,arXiv 2020,B,,,,
Towards Lossless Binary Convolutional Neural Networks Using Piecewise Approximation,arXiv 2020,B,,,,
RPR: Random Partition Relaxation for Training; Binary and Ternary Weight Neural Networks,arXiv 2020,B,,,,
MeliusNet: Can Binary Neural Networks Achieve MobileNet-level Accuracy?,arXiv 2020,B,,,,
Accelerating Binarized Neural Networks via Bit-Tensor-Cores in Turing GPUs,arXiv 2020,B,,,,
Distillation Guided Residual Learning for Binary Convolutional Neural Networks,arXiv 2020,B,,,,
How Does Batch Normalization Help Binary Training?,arXiv 2020,B,,,,
Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming[PyTorch],arXiv 2020,MP,OptN,PTQ,C/N,
A Product Engine for Energy-Efficient Execution of Binary Neural Networks Using Resistive Memories,VLSI-SoC 2019,,,,,
Deep Binary Reconstruction for Cross-Modal Hashing:fire:78,TMM 2019,,,,,
Compact Hash Code Learning With Binary Deep Neural Network,TM 2019,,,,,
Recursive Binary Neural Network Training Model for Efficient Usage of On-Chip Memory,TCSI 2019,,,,,
Xcel-RAM: Accelerating Binary Neural Networks in High-Throughput SRAM Compute Arrays,TCSI 2019,,,,,
An Energy-Efficient Reconfigurable Processor for Binary-and Ternary-Weight Neural Networks With Flexible Data Bit Width,JSSC 2019,,,,,
A Review of Binarized Neural Networks,Electronics 2019,,,,,
Binarized Neural Networks for Resource-Efficient Hashing with Minimizing Quantization Loss,IJCAI 2019,,,,,
Binarized Collaborative Filtering with Distilling Graph Convolutional Network,IJCAI 2019,,,,,
BiScaled-DNN: Quantizing Long-tailed Datastructures with Two Scale Factors for Deep Neural Networks,DAC 2019,,,,,
Latent Weights Do Not Exist: Rethinking Binarized Neural Network Optimization:fire:53,NeurIPS 2019,,,,,
A Mean Field Theory of Quantized Deep Networks: The Quantization-Depth Trade-Off[PyTorch]:star:12,NeurIPS 2019,,,,,Theory
Bit Efficient Quantization for Deep Neural Networks,NeurIPS 2019,Uni,Linear/Log,PTQ,C,
Communication-Efficient Distributed Learning via Lazily Aggregated Quantized Gradients,NeurIPS 2019,,,,,
Dimension-Free Bounds for Low-Precision Training,NeurIPS 2019,,Log,,,Theory
Double Quantization for Communication-Efficient Distributed Optimization,NeurIPS 2019,,,,,
Focused Quantization for Sparse CNNs,NeurIPS 2019,Uni,LQ,,C,
Generalization Error Analysis of Quantized Compressive Learning,NeurIPS 2019,,,,,Theory
Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks:fire:56,NeurIPS 2019,Uni,,PTQ,C/O/N,
MetaQuant: Learning to Quantize by Learning to Penetrate Non-differentiable Quantization[PyTorch]:star:48,NeurIPS 2019,B,,QAT,C,
Model Compression with Adversarial Robustness: A Unified Optimization Framework,NeurIPS 2019,Uni,,,C,
Normalization Helps Training of Quantized LSTM,NeurIPS 2019,B/T/Uni,,,C/N,
Post-training 4-bit quantization of convolution networks for rapid-deployment:fire:161[PyTorch]:star:163,NeurIPS 2019,T/Uni,,PTQ,C,
"Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations:fire:126",NeurIPS 2019,B,Randomized/Sign,,C,
Using Neuroevolved Binary Neural Networks to solve reinforcement learning environments,APCCAS 2019,,,,,
BinaryDenseNet: Developing an architecture for binary neural networks,ICCVW 2019,,,,,
Low-bit quantization of neural networks for efficient inference:fire:112,ICCV 2019,,,,,
Bayesian Optimized 1-Bit CNNs,ICCV 2019,B,,,C,
Data-Free Quantization Through Weight Equalization and Bias Correction:fire:135,ICCV 2019,Uni,Linear,PTQ,C,
Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks:fire:129,ICCV 2019,B/T/Uni,,,C,
HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision:fire:155,ICCV 2019,MP,Linear,QAT,C,
Proximal Mean-Field for Neural Network Quantization,ICCV 2019,B,,,C,
Unsupervised Neural Quantization for Compressed-Domain Similarity Search[PyTorch]:star:28,ICCV 2019,MP,LQ,,Image Retrieval,
Training Accurate Binary Neural Networks from Scratch,ICIP 2019,,,,,
Binarized Depthwise Separable Neural Network for Object Tracking in FPGA,GLSVLSI 2019,,,,,
Flow++: Improving Flow-Based Generative Models with Variational Dequantization and Architecture Design:fire:169[PyTorch]:star:152,ICML 2019,,,,,
Improving Neural Network Quantization without Retraining using Outlier Channel Splitting:fire:114[PyTorch]:star:80,ICML 2019,Uni,Linear,PTQ,C,
Lossless or Quantized Boosting with Integer Arithmetic,ICML 2019,,,,C,
SWALP : Stochastic Weight Averaging in Low-Precision Training[PyTorch]:star:52,ICML 2019,Uni,Linear,,C,
PXNOR: Perturbative Binary Neural Network,ROEDUNET 2019,,,,,
Learning channel-wise interactionsfor binary convolutional neural networks,CVPR 2019,,,,,
Circulant binary convolutional networks: Enhancing the performance of 1-bit dcnns with circulant back propagation,CVPR 2019,,,,,
Fighting quantization bias with bias,CVPR 2019,,,,,
A Main/Subsidiary Network Framework for Simplifying Binary Neural Networks,CVPR 2019,B,,,C,
Binary Ensemble Neural Network: More Bits per Network or More Networks per Bit?:fire:84,CVPR 2019,B,,,C,
Compressing Unknown Images With Product Quantizer for Efficient Zero-Shot Classification,CVPR 2019,MP,PQ/LQ,,C/ZSL/GZSL,
Deep Spherical Quantization for Image Search,CVPR 2019,,,,Image Search,
End-To-End Supervised Product Quantization for Image Search and Retrieval,CVPR 2019,,PQ,,Image Search/Retrieval,
Fully Quantized Network for Object Detection:fire:59,CVPR 2019,,,,,
HAQ: Hardware-Aware Automated Quantization With Mixed Precision:fire:305[PyTorch]:star:243,CVPR 2019,MP,Linear/K,QAT,C,
Learning Channel-Wise Interactions for Binary Convolutional Neural Networks,CVPR 2019,B,,,C,
Learning to Quantize Deep Networks by Optimizing Quantization Intervals With Task Loss:fire:168,CVPR 2019,T/Uni,LQ,,C,
Quantization Networks:fire:84,CVPR 2019,Uni,,QAT,C/O,
SeerNet: Predicting Convolutional Neural Network Feature-Map Sparsity Through Low-Bit Quantization,CVPR 2019,T/Uni,Linear,,C,
Simultaneously Optimizing Weight and Quantizer of Ternary Neural Network Using Truncated Gaussian Approximation,CVPR 2019,T,LQ,QAT,C,
Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation:fire:91,CVPR 2019,B,,,C/S,
Variational information distillation for knowledge transfer:fire:188,CVPR 2019,,,,,
Proxylessnas: Direct neural architecture search on target task and hardware,ICLR 2019,,,,,
Accumulation Bit-Width Scaling For Ultra-Low Precision Training Of Deep Networks,ICLR 2019,MP,,QAT,C,Theory
Analysis of Quantized Models,ICLR 2019,,,,,Theory
Defensive Quantization: When Efficiency Meets Robustness:fire:81,ICLR 2019,,,,C,
Double Viterbi: Weight Encoding for High Compression Ratio and Fast On-Chip Reconstruction for Deep Neural Network,ICLR 2019,B/T/Uni,,,C,CoDesign
From Hard to Soft: Understanding Deep Network Nonlinearities via Vector Quantization and Statistical Inference,ICLR 2019,MP,PQ,QAT,N,
Learning Recurrent Binary/Ternary Weights[PyTorch]:star:13,ICLR 2019,B/T,,QAT,C/N,
On the Universal Approximability and Complexity Bounds of Quantized ReLU Neural Networks,ICLR 2019,,,,C,Theory
Per-Tensor Fixed-Point Quantization of the Back-Propagation Algorithm,ICLR 2019,,,,C,
ProxQuant: Quantized Neural Networks via Proximal Operators:fire:56[PyTorch]:star:17,ICLR 2019,B,,QAT,C,
Relaxed Quantization for Discretized Neural Networks:fire:74,ICLR 2019,,LQ,,C,Stochastic
Understanding Straight-Through Estimator in Training Activation Quantized Neural Nets:fire:81,ICLR 2019,,,,C,Theory
Towards Fast and Energy-Efficient Binarized Neural Network Inference on FPGA,FPGA 2019,,,,,
Deep Neural Network Quantization via Layer-Wise Optimization using Limited Training Data[PyTorch]:star:30,AAAI 2019,Uni,,,C,
Efficient Quantization for Compact Neural Networks with Binary Weights and Low Bitwidth Activations,AAAI 2019,B,Linear/Log,QAT,C,
"Multi-Precision Quantized Neural Networks via Encoding Decomposition of {-1,+1}",AAAI 2019,MP,,,C/O,
Similarity Preserving Deep Asymmetric Quantization for Image Retrieval,AAAI 2019,,,QAT,Image Retrieval,
RBCN: Rectified Binary Convolutional Networks for Enhancing the Performance of 1-bit DCNNs,CoRR 2019,B,,,,
TentacleNet: A Pseudo-Ensemble Template for Accurate Binary Convolutional Neural Networks,CoRR 2019,B,,,,
Improved training of binary networks for human pose estimation and image recognition,CoRR 2019,B,,,,
Binarized Neural Architecture Search,CoRR 2019,,,,,
Matrix and tensor decompositions for training binary neural networks,CoRR 2019,,,,,
Back to Simplicity: How to Train Accurate BNNs from Scratch?,CoRR 2019,,,,,
MoBiNet: A Mobile Binary Network for Image Classification,arXiv 2019,,,,,
Training high-performance and large-scale deep neural networks with full 8-bit integers,arXiv 2019,,,,,
Knowledge distillation for optimization of quantized deep neural networks,arXiv 2019,,,,,
Accurate and compact convolutional neural networks with trained binarization,arXiv 2019,,,,,
Mixed precision training with 8-bit floating point,arXiv 2019,,,,,
Additive powers-of-two quantization: An efficient nonuniform discretization for neural networks,arXiv 2019,,,,,
Regularizing activation distribution for training binarized deep networks:fire:61,arXiv 2019,,,,,
The knowledge within: Methods for data-free model compression:fire:50,arXiv 2019,,,,,
Xnornet++: Improved binary neural networks,arXiv 2019,,,,,
Mixed Precision Quantization of ConvNets via Differentiable Neural Architecture Search,arXiv 2019,,,,,
QKD: Quantization-aware Knowledge Distillation,arXiv 2019,,,,,
daBNN: A Super Fast Inference Framework for Binary Neural Networks on ARM devices,arXiv 2019,,,,,
Towards Unified INT8 Training for Convolutional Neural Network,arXiv 2019,,,,,
BNN+: Improved Binary Network Training:fire:72,arXiv 2019,,,,,
Learned Step Size Quantization:fire:129,arXiv 2019,MP,LQ,QAT,C,
"Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization",arXiv 2019,Uni,Linear,PTQ,C,
An Energy-Efficient Architecture for Binary Weight Convolutional Neural Networks,TVLSI 2018,,,,,
FINN-R: An End-to-End Deep-Learning Framework for Fast Exploration of Quantized Neural Networks,TRETS 2018,,,,,
Inference of quantized neural networks on heterogeneous all-programmable devices,NE 2018,,,,,
A Deep Look into Logarithmic Quantization of Model Parameters in Neural Networks,IAIT 2018,,,,,
Deterministic Binary Filters for Convolutional Neural Networks,IJCAI 2018,,,,,
Planning in Factored State and Action Spaces with Learned Binarized Neural Network Transition Models,IJCAI 2018,,,,,
A Quantization-Friendly Separable Convolution for MobileNets,EMC2 2018,,,,,
Moonshine: Distilling with cheap convolutions,NeurIPS 2018,,,,,
A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication:fire:104,NeurIPS 2018,,,,,
GradiVeQ: Vector Quantization for Bandwidth-Efficient Gradient Aggregation in Distributed CNN Training,NeurIPS 2018,,PQ,,C,
Heterogeneous Bitwidth Binarization in Convolutional Neural Networks,NeurIPS 2018,MP,,,C,
HitNet: Hybrid Ternary Recurrent Neural Network,NeurIPS 2018,T/Uni,,,N,Stochastic
Scalable methods for 8-bit training of neural networks:fire:151[PyTorch]:star:191,NeurIPS 2018,Uni,,,C,
Training Deep Neural Networks with 8-bit Floating Point Numbers:fire:213,NeurIPS 2018,Uni,,,C,Stochastic
A survey of FPGA-based accelerators for convolutional neural networks,NCA 2018,,,,,
BitStream: Efficient Computing Architecture for Real-Time Low-Power Inference of Binary Neural Networks on CPUs,MM 2018,,,,,
FBNA: A Fully Binarized Neural Network Accelerator,FPL 2018,,,,,
Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm:fire:222[Caffe&pytorch]:star:138,ECCV 2018,B,,,,
LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep Neural Networks:fire:325[PyTorch]:star:207,ECCV 2018,B/Uni,LQ,QAT,C,
LSQ++: Lower running time and higher recall in multi-codebook quantization,ECCV 2018,MP,LQ,,C,
Learning Compression from limited unlabeled Data,ECCV 2018,Uni,,,C,
Product Quantization Network for Fast Image Retrieval,ECCV 2018,Uni,PQ,,Image Retrieval,
Quantization Mimic: Towards Very Tiny CNN for Object Detection:fire:55,ECCV 2018,Uni,,,O,
Quantized Densely Connected U-Nets for Efficient Landmark Localization:fire:105,ECCV 2018,,,,,
TBN: Convolutional Neural Network with Ternary Inputs and Binary Weights:fire:57,ECCV 2018,,,,,
Training Binary Weight Networks via Semi-Binary Decomposition,ECCV 2018,,,,,
Value-aware Quantization for Training and Inference of Neural Networks:fire:75,ECCV 2018,,,,,
Gap-8: A risc-v soc for ai at the edge of the iot,ASAP 2018,,,,,
Distilled binary neural network for monaural speech separation,IJCNN 2018,,,,,
Fast object detection based on binary deep convolution neural networks,IJCNN 2018,,,,,
Analysis and Implementation of Simple Dynamic Binary Neural Networks,IJCNN 2018,,,,,
SIGNSGD: compressed optimisation for non-convex problems:fire:393[PyTorch]:star:54,ICML 2018,,,,,Gradient
Quantization of Fully Convolutional Networks for Accurate Biomedical Image Segmentation:fire:59,CVPR 2018,,,,,
Explicit loss-error-aware quantization for low-bit deep neural networks:fire:67,CVPR 2018,,,,,
A biresolution spectral framework for product quantization,CVPR 2018,,,,,
Amc: Automl for model compression and acceleration on mobile devices:fire:814,CVPR 2018,,,,,
Effective Training of Convolutional Neural Networks with Low-bitwidth Weights and Activations,CVPR 2018,,,,,
Modulated convolutional networks,CVPR 2018,B,,,,
Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference:fire:1013,CVPR 2018,,,,,
SYQ: Learning Symmetric Quantization For Efficient Deep Neural Networks:fire:84[PyTorch]:star:31,CVPR 2018,T,,,,
Towards Effective Low-bitwidth Convolutional Neural Networks:fire:121,CVPR 2018,,,QAT,,
Two-Step Quantization for Low-bit Neural Networks:fire:72,CVPR 2018,,,,,
CLIP-Q: Deep Network Compression Learning by In-parallel Pruning-Quantization:fire:165,CVPR 2018,,,,,
BitFlow: Exploiting Vector Parallelism for Binary Neural Networks on CPU,IPDPS 2018,,,,,
Mixed Precision Training of Convolutional Neural Networks using Integer Operations:fire:117,ICLR 2018,,,,,
An empirical study of binary neural networks’ optimisation,ICLR 2018,,,,,
Adaptive Quantization of Neural Networks,ICLR 2018,,,,,
Alternating Multi-bit Quantization for Recurrent Neural Networks:fire:87,ICLR 2018,,,,,
Apprentice: Using Knowledge Distillation Techniques To Improve Low-Precision Network Accuracy:fire:208,ICLR 2018,,,,,
Energy-Constrained Compression for Deep Neural Networks via Weighted Sparse Projection and Layer Input Masking[PyTorch]:star:15,ICLR 2018,,,,,CoDesign
Loss-aware Weight Quantization of Deep Networks:fire:94,ICLR 2018,,,,,
Model compression via distillation and quantization:fire:353[PyTorch]:star:293,ICLR 2018,,,,,
Training and Inference with Integers in Deep Neural Networks:fire:231[[tensorflow]]:star:132,ICLR 2018,,,,,
Variational Network Quantization:fire:58,ICLR 2018,,,,,
WRPN: Wide Reduced-Precision Networks:fire:180,ICLR 2018,,,,,
Adaptive Quantization for Deep Neural Networ:fire:70,AAAI 2018,,,,,
Deep Neural Network Compression with Single and Multiple Level Quantization:fire:65[PyTorch]:star:20,AAAI 2018,,,,,
Distributed Composite Quantization,AAAI 2018,,,,,
Extremely Low Bit Neural Network: Squeeze the Last Bit Out with ADMM:fire:207,AAAI 2018,,,,,
From Hashing to CNNs: Training Binary Weight Networks via Hashing:fire:62,AAAI 2018,B,,,,
Product Quantized Translation for Fast Nearest Neighbor Search,AAAI 2018,,,,,
Quantized Memory-Augmented Neural Networks,AAAI 2018,,,,,
ReBNet: Residual Binarized Neural Network,ISFPCCM 2018,,,,,
LightNN: Filling the Gap between Conventional Deep Neural Networks and Binarized Networks,CoRR 2018,,,,,
BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights,CoRR 2018,,,,,
Learning low precision deep neural networks through regularization,arXiv 2018,,,,,
Blended coarse gradient descent for full quantization of deep neural networks:fire:48,arXiv 2018,,,,,
XNOR Neural Engine: A Hardware Accelerator IP for 21.6-fJ/op Binary Neural Network Inference:fire:72,arXiv 2018,,,,,
A Survey on Methods and Theories of Quantized Neural Networks:fire:128,arXiv 2018,,,,,
Quantizing Convolutional Neural Networks for Low-Power High-Throughput Inference Engines,arXiv 2018,,,,,
Discovering low-precision networks close to full-precision networks for efficient embedded inference:fire:83,arXiv 2018,,,,,
On periodic functions as regularizers for quantization of neural networks,arXiv 2018,,,,,
Rethinking floating point for deep learning:fire:95,arXiv 2018,,,,,
Quantizing deep convolutional networks for efficient inference: A whitepaper:fire:425,arXiv 2018,,,,,
Quantization for rapid deployment of deep neural networks,arXiv 2018,,,,,
Simultaneously optimizing weight and quantizer of ternary neural network using truncated gaussian approximation,arXiv 2018,,,,,
Uniq: Uniform noise injection for non-uniform quantization of neural networks,arXiv 2018,,,,,
Training Competitive Binary Neural Networks from Scratch,arXiv 2018,,,,,
Joint Neural Architecture Search and Quantization,arXiv 2018,,,,,
BinaryRelax: A Relaxation Approach For Training Deep Neural Networks With Quantized Weights:fire:55,arXiv 2018,,,,,
Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients:fire:1209[PyTorch],arXiv 2018,Uni,,,,
Espresso: Efficient Forward Propagation for BCNNs,arXiv 2018,,,,,CoDesign
Mixed Precision Training:fire:601,arXiv 2018,Uni,,,,
PACT: Parameterized Clipping Activation for Quantized Neural Networks:fire:341,arXiv 2018,,,,,
Terngrad: Ternary gradients to reduce communication in distributed deep learning:fire:649,NeurIPS 2017,,,,,
QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding:fire:696,NeurIPS 2017,,,,,Gradient
Towards Accurate Binary Convolutional Neural Network:fire:193[TensorFlow]:star:49,NeurIPS 2017,B,,,,
Training Quantized Nets: A Deeper Understanding:fire:134,NeurIPS 2017,,,,,
Learning Accurate Low-Bit Deep Neural Networks with Stochastic Quantization[Caffe],BMVC 2017,,,,,
Performance guaranteed network acceleration via high-order residual quantization,ICCV 2017,,,,,
Binarized Convolutional Landmark Localizers for Human Pose Estimation and Face Alignment with Limited Resources:fire: 130[PyTorch]:star:207,ICCV 2017,B,,,,
Performance Guaranteed Network Acceleration via High-Order Residual Quantization:fire:55,ICCV 2017,,,,,
Binary Deep Neural Networks for Speech Recognition,Interspeech 2017,B,,,,
Ternary neural networks for resource-efficient AI applications,IJCNN 2017,,,,,
Fixed-point optimization of deep neural networks with adaptive step size retraining,ICASSP 2017,MP,,,,
Deep Learning with Low Precision by Half-wave Gaussian Quantization:fire:288[Caffe]:star:118,CVPR 2017,,,,,
Fixed-point Factorized Networks,CVPR 2017,T,,,,Factor
Local Binary Convolutional Neural Networks:star:94:fire:156,CVPR 2017,,,,,
Network Sketching: Exploiting Binary Structure in Deep CNNs:fire:71,CVPR 2017,B,,,,
Weighted-Entropy-Based Quantization for Deep Neural Networks:fire:144,CVPR 2017,,Non,,,
A GPU-Outperforming FPGA Accelerator Architecture for Binary Convolutional Neural Networks,DC 2017,B,,,,CoDesign
On-Chip Memory Based Binarized Convolutional Deep Neural Network Applying Batch Normalization Free Technique on an FPGA,IPDPSW 2017,,,,,CoDesign
Incremental Network Quantization: Towards Lossless CNNs with Low-Precision Weights:fire:607[PyTorch]:star:181,ICLR 2017,T/Uni,Log,QAT,C,
Learning Discrete Weights Using the Local Reparameterization Trick:fire:61,ICLR 2017,,,,,Stochastic
Loss-aware Binarization of Deep Networks:fire:119[PyTorch]:star:18,ICLR 2017,B,,,,
Soft Weight-Sharing for Neural Network Compression:fire:222:star:18,ICLR 2017,B,,,,
Towards the Limit of Network Quantization:fire:114,ICLR 2017,Uni,,,,
"FINN: A Framework for Fast, Scalable Binarized Neural Network Inference:fire:463",FPGA 2017,B,,,,
How to train a compact binary neural network with high accuracy?:fire:205,AAAI 2017,,,,,
Adaptive Quantization for Deep Neural Network:fire:67,AAAI 2017,MP,,,,
The high-dimensional geometry of binary neural networks,CoRR 2017,,,,,
BMXNet: An Open-Source Binary Neural Network Implementation Based on MXNet,CoRR 2017,,,,,
Deep learning binary neural network on an FPGA,arXiv 2017,B,,,,
FP-BNN: Binarized neural network on FPGA:126:,arXiv 2017,B,,,,CoDesign
Accelerating Deep Convolutional Networks using low-precision and sparsity:fire:111,arXiv 2017,T,,,,
Bit-regularized optimization of neural nets,arXiv 2017,,,,,
Balanced Quantization: An Effective and Efficient Approach to Quantized Neural Networks,arXiv 2017,,,,,
Learning deep binary descriptor with multi-quantization:fire:97,arXiv 2017,,,,,
Gxnor-net: Training deep neural networks with ternary weights and activations without full-precision memory under a unified discretization framework,arXiv 2017,,,,,
Soft-to-hard vector quantization for end-to-end learning compressible representations,arXiv 2017,,,,,
ShiftCNN: Generalized Low-Precision Architecture for Inference of Convolutional Neural Networks[TensorFlow]:star:53,arXiv 2017,,,,,
Ternary Neural Networks with Fine-Grained Quantization:fire:71,arXiv 2017,T,,,,
Trained Ternary Quantization:fire:734,arXiv 2017,T,,,,
Decision making with quantized priors leads to discrimination,JPROC 2016,,,,,
Communication quantization for data-parallel training of deep neural networks:fire:130,MLHPC 2016,,,,,
XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks:fire:3469[PyTorch]:star:807,ECCV 2016,B,,,,
Overcoming challenges in fixed point training of deep convolutional networks,ICMLW 2016,,,,,
Fixed point quantization of deep convolutional networks:fire:696,ICML 2016,,,,,
"Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding:fire:5045",CVPR 2016,,,,,
Quantized convolutional neural networks for mobile devices:fire:270,CVPR 2016,,,,,
Fixed-point Performance Analysis of Recurrent Neural Networks:fire:67,arXiv 2016,,,,,
Qsgd: Randomized quantization for communication-optimal stochastic gradient descent:fire:801,arXiv 2016,,,,,
Effective quantization methods for recurrent neural networks:fire:62,arXiv 2016,,,,,
Sigma delta quantized networks,arXiv 2016,,,,,
Recurrent neural networks with limited numerical precision:fire:65,arXiv 2016,,,,,
Training bit fully convolutional network for fast semantic segmentation,arXiv 2016,,,,,
Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations:fire:1347,arXiv 2016,,,,,
Convolutional neural networks using logarithmic data representation:fire:320,arXiv 2016,,,,,
Layer normalization:fire:4125,arXiv 2016,,,,,
Binarized Neural Networks on the ImageNet Classification Task,arXiv 2016,B,,,,
Binarized Neural Networks: Training Neural Networks with Weights and Activations Constrained to +1 or −1:fire:1574[PyTorch]:star:252,arXiv 2016,B,,,,
Deep neural networks are robust to weight binarization and other non-linear distortions:fire:77,arXiv 2016,,,,,
Neural Networks with Few Multiplications:fire:258[PyTorch]:star:81,arXiv 2016,T,,,,
Ternary weight networks:fire:647[Caffe]:star:63,arXiv 2016,T,,,,
Batch normalization: Accelerating deep network training by reducing internal covariate shift:fire:32893,PMLR 2015,,,,,
BinaryConnect: Training Deep Neural Networks with binary weights during propagations:fire:2267[PyTorch]:star:344,NeurIPS 2015,B,,,,
Bitwise Neural Networks:fire:191,ICML 2015,B,,,,
Compressing neural networks with hashing trick:fire:887,ICML 2015,,,,,
Deep Learning with Limited Numerical Precision:fire:1378,ICML 2015,Uni,,,,
Fixed point optimization of deep convolutional neural networks for object recognition:fire:226,ICASSP 2015,,,,,
8-Bit Approximations for Parallelism in Deep Learning:fire:114,ICLR 2015,,,,,
Training deep neural networks with low precision multiplications:fire:498,ICLR 2015,,,,,
Rounding methods for neural networks with low resolution synaptic weights,arXiv 2015,,,,,
Training Binary Multilayer Neural Networks for Image Classification using Expectation Backpropagation:fire:50,arXiv 2015,,,,,
Resiliency of Deep Neural Networks under quantizations:fire:123,arXiv 2015,,,,,
"Fixed-point feedforward deep neural network design using weights +1, 0, and −1:fire:269",SiPS 2014,,,,,
Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights:fire:190,NeurIPS 2014,,,,,Stochastic
1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns:fire:679,Interspeech 2014,,,,,
Compressing deep convolutional networks using vector quantization:fire:981,arXiv 2014,,,,,
Lowrank matrix factorization for deep neural network training with high-dimensional output targets:fire:563,ICASSP 2013,,,,,
Estimating or propagating gradients through stochastic neurons for conditional computation:fire:1346,arXiv 2013,,,,,
Product quantization for nearest neighbor search:fire:2268,TPAMI 2010,,,,,
An introduction to natural computation:fire:309,MITPress 1999,,,,,